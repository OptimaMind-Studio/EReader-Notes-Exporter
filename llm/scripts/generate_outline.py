#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦ä¹ å¤§çº²ç”Ÿæˆå·¥å…·
ä»ç¬”è®° CSV æ–‡ä»¶ä¸­æŒ‰ç« èŠ‚åˆ†ç»„ï¼Œä½¿ç”¨ Gemini API ç”Ÿæˆå­¦ä¹ å¤§çº²
"""

import sys
import os
import csv
import json
import time
import re
import html
import argparse
import subprocess
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from collections import defaultdict
from google import genai

# å¯¼å…¥ prompt æ¨¡æ¿
try:
    # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ—¶
    from llm.prompts import OUTLINE_PROMPT_TEMPLATE
except ImportError:
    # ä» llm ç›®å½•è¿è¡Œæ—¶
    from prompts import OUTLINE_PROMPT_TEMPLATE


class OutlineGenerator:
    """ä½¿ç”¨ Gemini API ç”Ÿæˆå­¦ä¹ å¤§çº²"""
    
    PROMPT_TEMPLATE = OUTLINE_PROMPT_TEMPLATE
    
    def __init__(self, api_key: Optional[str] = None, role: str = "å­¦ä¹ è€…"):
        """
        åˆå§‹åŒ– Gemini API å®¢æˆ·ç«¯
        
        Args:
            api_key: Gemini API å¯†é’¥ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            role: è§’è‰²ï¼ˆé»˜è®¤ä¸º"å­¦ä¹ è€…"ï¼‰
        """
        if api_key is None:
            api_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            raise ValueError(
                "è¯·æä¾› Gemini API å¯†é’¥ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ï¼š\n"
                "1. ä½œä¸ºå‚æ•°ä¼ å…¥ï¼šOutlineGenerator(api_key='your_key')\n"
                "2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport GEMINI_API_KEY='your_api_key' æˆ– export GOOGLE_API_KEY='your_api_key'"
            )
        
        self.client = genai.Client(api_key=api_key)
        self.role = role
    
    def _clean_json_string(self, json_str: str) -> str:
        """
        æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„æ§åˆ¶å­—ç¬¦
        
        Args:
            json_str: åŸå§‹ JSON å­—ç¬¦ä¸²
        
        Returns:
            æ¸…ç†åçš„ JSON å­—ç¬¦ä¸²
        """
        # ç§»é™¤å­—ç¬¦ä¸²å€¼å¤–çš„æ§åˆ¶å­—ç¬¦ï¼ˆ\x00-\x1Fï¼Œé™¤äº† \n, \r, \tï¼‰
        # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŒºåˆ†å­—ç¬¦ä¸²å€¼å†…å¤–çš„æ§åˆ¶å­—ç¬¦
        
        # ç®€å•æ–¹æ³•ï¼šå°è¯•ä¿®å¤å¸¸è§çš„æ§åˆ¶å­—ç¬¦é—®é¢˜
        # ç§»é™¤å­—ç¬¦ä¸²å€¼å¤–çš„æ§åˆ¶å­—ç¬¦
        lines = json_str.split('\n')
        cleaned_lines = []
        in_string = False
        escape_next = False
        
        for line in lines:
            cleaned_line = []
            for char in line:
                if escape_next:
                    cleaned_line.append(char)
                    escape_next = False
                    continue
                
                if char == '\\':
                    escape_next = True
                    cleaned_line.append(char)
                    continue
                
                if char == '"' and not escape_next:
                    in_string = not in_string
                    cleaned_line.append(char)
                    continue
                
                # å¦‚æœæ˜¯æ§åˆ¶å­—ç¬¦ä¸”ä¸åœ¨å­—ç¬¦ä¸²å€¼ä¸­ï¼Œè·³è¿‡
                if not in_string and ord(char) < 32 and char not in ['\n', '\r', '\t']:
                    continue
                
                cleaned_line.append(char)
            
            cleaned_lines.append(''.join(cleaned_line))
        
        return '\n'.join(cleaned_lines)
    
    def generate_outline(self, mark_notes: str, review_notes: str, max_retries: int = 3) -> Dict[str, str]:
        """
        ç”Ÿæˆå­¦ä¹ å¤§çº²
        
        Args:
            mark_notes: åˆ’çº¿ç¬”è®°ï¼ˆåŒ…å«ç« èŠ‚æ ‡é¢˜å’Œåˆ’çº¿æ–‡æœ¬ï¼‰
            review_notes: ç‚¹è¯„ç¬”è®°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå½“ HTML è§£æå¤±è´¥æ—¶ï¼‰
        
        Returns:
            åŒ…å« 'markdown' å’Œ 'html' çš„å­—å…¸
        """
        # æ›¿æ¢ prompt æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        prompt = self.PROMPT_TEMPLATE.replace("{{åˆ’çº¿ç¬”è®°}}", mark_notes)
        prompt = prompt.replace("{{ç‚¹è¯„ç¬”è®°}}", review_notes)
        
        last_error = None
        last_response_text = None
        
        for attempt in range(max_retries):
            try:
                # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œæ‰“å°é‡è¯•ä¿¡æ¯
                if attempt > 0:
                    print(f"  ğŸ”„ é‡è¯•ç¬¬ {attempt} æ¬¡...")
                
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash-001',
                    contents=prompt,
                )
                
                # è·å–å“åº”æ–‡æœ¬
                if hasattr(response, 'text'):
                    response_text = response.text
                elif hasattr(response, 'candidates') and len(response.candidates) > 0:
                    response_text = response.candidates[0].content.parts[0].text
                else:
                    response_text = str(response)
                
                response_text = response_text.strip()
                last_response_text = response_text
                
                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œæå– HTML éƒ¨åˆ†
                # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼ˆåŒ…æ‹¬ ```html, ```, ç­‰ï¼‰
                if response_text.startswith('```'):
                    lines = response_text.split('\n')
                    start_idx = 1
                    end_idx = len(lines)
                    for i, line in enumerate(lines):
                        if line.strip().startswith('```') and i > 0:
                            end_idx = i
                            break
                    response_text = '\n'.join(lines[start_idx:end_idx])
                
                # ç§»é™¤æ‰€æœ‰å‰©ä½™çš„ markdown ä»£ç å—æ ‡è®°
                response_text = re.sub(r'^```[a-z]*\n?', '', response_text, flags=re.MULTILINE)
                response_text = re.sub(r'\n?```$', '', response_text, flags=re.MULTILINE)
                response_text = response_text.strip()
                
                # ç§»é™¤å‰åå¼•å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if response_text.startswith('"') and response_text.endswith('"'):
                    response_text = response_text[1:-1]
                if response_text.startswith("'") and response_text.endswith("'"):
                    response_text = response_text[1:-1]
                response_text = response_text.strip()
                
                # éªŒè¯å’Œæ¸…ç† HTML
                html_content = self._validate_and_clean_html(response_text)
                
                if html_content:
                    # ä» HTML ç”Ÿæˆ markdownï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                    markdown_content = self._html_to_markdown(html_content)
                    
                    return {
                        'markdown': markdown_content,
                        'html': html_content
                    }
                else:
                    # HTML éªŒè¯å¤±è´¥ï¼Œé‡è¯•
                    if attempt < max_retries - 1:
                        last_error = "HTML æ ¼å¼éªŒè¯å¤±è´¥"
                        print(f"  âš ï¸  HTML æ ¼å¼éªŒè¯å¤±è´¥ï¼Œé‡è¯•...")
                        time.sleep(1)
                        continue
                    else:
                        # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                        error_text = response_text[:1000]
                        error_text = html.escape(error_text)
                        error_text = re.sub(r'```[a-z]*\n?', '', error_text)
                        error_text = re.sub(r'\n?```', '', error_text)
                        
                        return {
                            'markdown': f"HTML æ ¼å¼éªŒè¯å¤±è´¥\n\nåŸå§‹å“åº”ï¼š\n{response_text[:1000]}",
                            'html': f"<html><body><p>HTML æ ¼å¼éªŒè¯å¤±è´¥</p><pre>{error_text}</pre></body></html>"
                        }
            
            except Exception as e:
                last_error = str(e)
                error_msg = f"ç”Ÿæˆå¤§çº²æ—¶å‡ºé”™ï¼š{str(e)}"
                print(f"  âš ï¸  {error_msg}")
                
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–å¯é‡è¯•çš„é”™è¯¯ï¼Œä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­é‡è¯•
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç­‰å¾… 1 ç§’åé‡è¯•
                    continue
                
                # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                error_text = last_response_text[:1000] if last_response_text else 'æ— å“åº”'
                error_text = html.escape(error_text)
                error_text = re.sub(r'```[a-z]*\n?', '', error_text)
                error_text = re.sub(r'\n?```', '', error_text)
                
                return {
                    'markdown': error_msg,
                    'html': f"<html><body><p>{error_msg}</p><pre>{error_text}</pre></body></html>"
                }
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åçš„é”™è¯¯ä¿¡æ¯
        error_text = last_response_text[:1000] if last_response_text else 'æ— å“åº”'
        error_text = html.escape(error_text)
        error_text = re.sub(r'```[a-z]*\n?', '', error_text)
        error_text = re.sub(r'\n?```', '', error_text)
        
        return {
            'markdown': f"ç”Ÿæˆå¤§çº²å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ï¼š{last_error}\n\nåŸå§‹å“åº”ï¼š\n{last_response_text[:1000] if last_response_text else 'æ— å“åº”'}",
            'html': f"<html><body><p>ç”Ÿæˆå¤§çº²å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ï¼š{last_error}</p><pre>{error_text}</pre></body></html>"
        }
    
    def _validate_and_clean_html(self, html_text: str) -> Optional[str]:
        """
        éªŒè¯å’Œæ¸…ç† HTML æ–‡æœ¬
        
        Args:
            html_text: åŸå§‹ HTML æ–‡æœ¬
        
        Returns:
            æ¸…ç†åçš„ HTML æ–‡æœ¬ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å› None
        """
        if not html_text or not html_text.strip():
            return None
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„ HTML æ ‡ç­¾
        if not re.search(r'<[hH][1-6]', html_text) and not re.search(r'<[pP]', html_text):
            # å¦‚æœæ²¡æœ‰ HTML æ ‡ç­¾ï¼Œå¯èƒ½ä¸æ˜¯ HTML æ ¼å¼
            print(f"  âš ï¸  å“åº”ä¸­æœªæ‰¾åˆ° HTML æ ‡ç­¾")
            return None
        
        # æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼ˆä½†ä¿ç•™å­—ç¬¦ä¸²å€¼ä¸­çš„åˆæ³•è½¬ä¹‰å­—ç¬¦ï¼‰
        # ç§»é™¤å­—ç¬¦ä¸²å€¼å¤–çš„æ§åˆ¶å­—ç¬¦
        cleaned_html = self._clean_html_string(html_text)
        
        # ç¡®ä¿ HTML æ˜¯å®Œæ•´çš„ï¼ˆå¦‚æœæ²¡æœ‰ html/body æ ‡ç­¾ï¼Œæ·»åŠ å®ƒä»¬ï¼‰
        if not re.search(r'<html', cleaned_html, re.IGNORECASE):
            # å¦‚æœæ²¡æœ‰å®Œæ•´çš„ HTML ç»“æ„ï¼Œåªè¿”å›å†…å®¹éƒ¨åˆ†
            # è°ƒç”¨è€…ä¼šè´Ÿè´£åŒ…è£…
            return cleaned_html
        
        return cleaned_html
    
    def _clean_html_string(self, html_str: str) -> str:
        """
        æ¸…ç† HTML å­—ç¬¦ä¸²ä¸­çš„æ§åˆ¶å­—ç¬¦
        
        Args:
            html_str: åŸå§‹ HTML å­—ç¬¦ä¸²
        
        Returns:
            æ¸…ç†åçš„ HTML å­—ç¬¦ä¸²
        """
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº† \n, \r, \tï¼‰
        result = []
        for char in html_str:
            if ord(char) < 32 and char not in ['\n', '\r', '\t']:
                continue
            result.append(char)
        
        return ''.join(result)
    
    def _html_to_markdown(self, html_content: str) -> str:
        """
        ä» HTML ç”Ÿæˆç®€åŒ–çš„ Markdownï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        
        Args:
            html_content: HTML å†…å®¹
        
        Returns:
            Markdown æ ¼å¼çš„æ–‡æœ¬
        """
        # ç®€å•çš„ HTML åˆ° Markdown è½¬æ¢
        # ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        markdown = html_content
        
        # æ›¿æ¢æ ‡é¢˜æ ‡ç­¾ - ä½¿ç”¨ lambda å‡½æ•°æ¥å¤„ç†åå‘å¼•ç”¨
        def replace_header(match):
            level = int(match.group(1))
            text = match.group(2)
            return f'\n{"#" * level} {text}\n'
        
        markdown = re.sub(r'<h([1-6])>(.*?)</h\1>', replace_header, markdown, flags=re.IGNORECASE | re.DOTALL)
        
        # æ›¿æ¢æ®µè½æ ‡ç­¾
        markdown = re.sub(r'<p>(.*?)</p>', r'\1\n\n', markdown, flags=re.IGNORECASE | re.DOTALL)
        
        # æ›¿æ¢åŠ ç²—æ ‡ç­¾
        markdown = re.sub(r'<strong>(.*?)</strong>', r'**\1**', markdown, flags=re.IGNORECASE | re.DOTALL)
        markdown = re.sub(r'<b>(.*?)</b>', r'**\1**', markdown, flags=re.IGNORECASE | re.DOTALL)
        
        # ç§»é™¤å…¶ä»– HTML æ ‡ç­¾
        markdown = re.sub(r'<[^>]+>', '', markdown)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        
        return markdown.strip()
    
    def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        if hasattr(self, 'client'):
            try:
                self.client.close()
            except:
                pass


def read_csv_file(csv_file: str) -> List[Dict[str, str]]:
    """
    è¯»å– CSV æ–‡ä»¶
    
    Args:
        csv_file: CSV æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ•°æ®è¡Œåˆ—è¡¨
    """
    rows = []
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # åªä¿ç•™æœ‰ markText çš„è¡Œ
                if row.get('markText', '').strip():
                    rows.append(row)
        return rows
    except Exception as e:
        print(f"é”™è¯¯ï¼šè¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")
        return []


def group_by_chapters(rows: List[Dict[str, str]]) -> Dict[int, List[Dict[str, str]]]:
    """
    æŒ‰ç« èŠ‚åˆ†ç»„
    
    Args:
        rows: CSV æ•°æ®è¡Œåˆ—è¡¨
    
    Returns:
        æŒ‰ chapterUid åˆ†ç»„çš„å­—å…¸
    """
    chapters = defaultdict(list)
    
    for row in rows:
        chapter_uid = row.get('chapterUid', '').strip()
        if chapter_uid:
            try:
                uid = int(chapter_uid)
                chapters[uid].append(row)
            except (ValueError, TypeError):
                continue
    
    return dict(sorted(chapters.items()))


def find_book_id_by_title(csv_file: str, book_title: str) -> Optional[str]:
    """
    æ ¹æ®ä¹¦ååœ¨ CSV æ–‡ä»¶ä¸­æŸ¥æ‰¾ bookId
    æ”¯æŒç²¾ç¡®åŒ¹é…å’Œéƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœä¹¦ååŒ…å«åœ¨ CSV çš„ title å­—æ®µä¸­ï¼Œæˆ– CSV çš„ title åŒ…å«åœ¨è¾“å…¥çš„ä¹¦åä¸­ï¼‰
    
    Args:
        csv_file: CSV æ–‡ä»¶è·¯å¾„
        book_title: ä¹¦å
    
    Returns:
        bookIdï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    try:
        book_title_lower = book_title.strip().lower()
        exact_match = None
        partial_matches = []
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                title = row.get('title', '').strip()
                title_lower = title.lower()
                book_id = row.get('bookId', '').strip()
                
                # ç²¾ç¡®åŒ¹é…
                if title == book_title or title_lower == book_title_lower:
                    exact_match = book_id
                    break
                
                # éƒ¨åˆ†åŒ¹é…ï¼šè¾“å…¥çš„ä¹¦ååŒ…å«åœ¨ CSV çš„ title ä¸­ï¼Œæˆ– CSV çš„ title åŒ…å«åœ¨è¾“å…¥çš„ä¹¦åä¸­
                if book_title_lower in title_lower or title_lower in book_title_lower:
                    partial_matches.append((title, book_id))
        
        # ä¼˜å…ˆè¿”å›ç²¾ç¡®åŒ¹é…
        if exact_match:
            return exact_match
        
        # å¦‚æœæœ‰éƒ¨åˆ†åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€ç›¸å…³çš„ï¼‰
        if partial_matches:
            # ä¼˜å…ˆè¿”å›åŒ…å«è¾“å…¥ä¹¦åæœ€çŸ­çš„é‚£ä¸ªï¼ˆæ›´ç²¾ç¡®ï¼‰
            partial_matches.sort(key=lambda x: len(x[0]))
            return partial_matches[0][1]
        
        return None
    except Exception as e:
        print(f"é”™è¯¯ï¼šè¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")
        return None


def find_book_by_id(csv_file: str, book_id: str) -> Optional[Dict[str, str]]:
    """
    æ ¹æ® bookId åœ¨ CSV æ–‡ä»¶ä¸­æŸ¥æ‰¾ä¹¦ç±ä¿¡æ¯
    
    Args:
        csv_file: CSV æ–‡ä»¶è·¯å¾„
        book_id: ä¹¦ç±ID
    
    Returns:
        ä¹¦ç±ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('bookId', '').strip() == book_id:
                    return {
                        'bookId': book_id,
                        'title': row.get('title', '').strip(),
                        'author': row.get('author', '').strip(),
                        'categories': row.get('categories', '').strip()
                    }
        return None
    except Exception as e:
        print(f"é”™è¯¯ï¼šè¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")
        return None


def fetch_notes_data(book_id: Optional[str] = None, book_name: Optional[str] = None, project_root: Path = None) -> bool:
    """
    é‡æ–° fetch ç¬”è®°æ•°æ®
    
    Args:
        book_id: ä¹¦ç±IDï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™åª fetch è¯¥ä¹¦ç±ï¼‰
        book_name: ä¹¦åï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™åª fetch è¯¥ä¹¦ç±ï¼Œä¼˜å…ˆäº book_idï¼‰
        project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
    
    Returns:
        å¦‚æœæˆåŠŸè¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if project_root is None:
        script_dir = Path(__file__).parent  # llm/scripts
        project_root = script_dir.parent.parent  # é¡¹ç›®æ ¹ç›®å½•
    
    fetch_script = project_root / "wereader" / "fetch.py"
    
    if not fetch_script.exists():
        print(f"âš ï¸  è­¦å‘Šï¼šfetch è„šæœ¬ä¸å­˜åœ¨: {fetch_script}")
        print(f"   è¯·ç¡®ä¿ wereader/fetch.py æ–‡ä»¶å­˜åœ¨")
        return False
    
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨é‡æ–° fetch ç¬”è®°æ•°æ®...")
    print(f"{'='*60}")
    
    args = [sys.executable, str(fetch_script)]
    if book_name:
        args.extend(['--book-name', book_name])
        print(f"å¤„ç†ä¹¦ç±: {book_name}")
    elif book_id:
        args.extend(['--book-id', book_id])
        print(f"å¤„ç†ä¹¦ç± ID: {book_id}")
    else:
        print(f"å¤„ç†æ‰€æœ‰ä¹¦ç±")
    
    try:
        result = subprocess.run(
            args,
            cwd=str(project_root),
            check=False,
            capture_output=False  # æ˜¾ç¤ºè¾“å‡º
        )
        if result.returncode == 0:
            print(f"âœ“ Fetch å®Œæˆ")
            return True
        else:
            print(f"âš ï¸  Fetch å¤±è´¥ï¼ˆé€€å‡ºç : {result.returncode}ï¼‰")
            return False
    except Exception as e:
        print(f"âŒ Fetch æ‰§è¡Œå‡ºé”™: {e}")
        return False


def process_csv_file(book_id: Optional[str] = None, book_title: Optional[str] = None, output_file: Optional[str] = None, api_key: Optional[str] = None, role: str = "å­¦ä¹ è€…", fetch_data: bool = False):
    """
    å¤„ç† CSV æ–‡ä»¶ï¼Œç”Ÿæˆå­¦ä¹ å¤§çº²
    
    Args:
        book_id: ä¹¦ç±IDï¼ˆä¸ book_title äºŒé€‰ä¸€ï¼‰
        book_title: ä¹¦åï¼ˆä¸ book_id äºŒé€‰ä¸€ï¼‰
        output_file: è¾“å‡ºçš„ Markdown æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆ
        api_key: Gemini API å¯†é’¥
        role: è§’è‰²ï¼ˆé»˜è®¤ä¸º"å­¦ä¹ è€…"ï¼‰
        fetch_data: æ˜¯å¦å…ˆé‡æ–° fetch ç¬”è®°æ•°æ®ï¼ˆé»˜è®¤ Falseï¼‰
    """
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = Path(__file__).parent  # llm/scripts
    project_root = script_dir.parent.parent  # é¡¹ç›®æ ¹ç›®å½•
    
    # å¦‚æœå¯ç”¨äº† fetch_dataï¼Œå…ˆé‡æ–° fetch ç¬”è®°æ•°æ®
    if fetch_data:
        # ä¼˜å…ˆä½¿ç”¨ book_nameï¼ˆbook_titleï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ book_id
        if not fetch_notes_data(book_id=book_id, book_name=book_title, project_root=project_root):
            print(f"\nâš ï¸  è­¦å‘Šï¼šfetch æ•°æ®å¤±è´¥ï¼Œå°†ä½¿ç”¨å·²æœ‰çš„ç¬”è®°æ–‡ä»¶")
        else:
            print(f"\nâœ“ æ•°æ®å·²æ›´æ–°ï¼Œç»§ç»­ç”Ÿæˆ outline...\n")
    
    # é»˜è®¤è·¯å¾„
    notebooks_csv = project_root / "wereader" / "output" / "fetch_notebooks_output.csv"
    notes_dir = project_root / "wereader" / "output" / "notes"
    
    # 1. ç¡®å®š bookId
    book_info = None
    
    if book_id:
        # å¦‚æœæä¾›äº† bookIdï¼Œç›´æ¥ä½¿ç”¨
        print(f"ä½¿ç”¨ bookId: {book_id}")
        book_info = find_book_by_id(str(notebooks_csv), book_id)
        if not book_info:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ° bookId '{book_id}' å¯¹åº”çš„ä¹¦ç±")
            return
        book_id = book_info['bookId']
        book_title_display = book_info['title']
    elif book_title:
        # å¦‚æœæä¾›äº†ä¹¦åï¼ŒæŸ¥æ‰¾å¯¹åº”çš„ bookId
        print(f"æ­£åœ¨æŸ¥æ‰¾ä¹¦åï¼š{book_title}")
        book_id = find_book_id_by_title(str(notebooks_csv), book_title)
        if not book_id:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°ä¹¦å '{book_title}' å¯¹åº”çš„ bookId")
            return
        book_info = find_book_by_id(str(notebooks_csv), book_id)
        book_title_display = book_title
    else:
        print("é”™è¯¯ï¼šå¿…é¡»æä¾› bookId æˆ– book_title ä¹‹ä¸€")
        return
    
    print(f"æ‰¾åˆ°ä¹¦ç±: {book_title_display} (ID: {book_id})\n")
    
    # 2. æ„å»º CSV æ–‡ä»¶è·¯å¾„
    csv_file = notes_dir / f"{book_id}.csv"
    
    if not csv_file.exists():
        print(f"é”™è¯¯ï¼šç¬”è®°æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    
    # è¯»å– CSV æ–‡ä»¶
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {csv_file}")
    rows = read_csv_file(str(csv_file))
    
    if not rows:
        print("é”™è¯¯ï¼šæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return
    
    print(f"å…±è¯»å– {len(rows)} è¡Œæ•°æ®")
    
    # è·å–é¢†åŸŸå’Œä¹¦ç±ä¿¡æ¯
    field = ''
    book_title = ''
    for row in rows:
        if not field:
            field = row.get('categories', '').strip()
        if not book_title:
            book_title = row.get('title', '').strip()
        if field and book_title:
            break
    
    if not field:
        field = "æœªçŸ¥é¢†åŸŸ"
    if not book_title:
        book_title = "æœªçŸ¥ä¹¦ç±"
    
    print(f"ä¹¦ç±: {book_title}")
    print(f"é¢†åŸŸ: {field}\n")
    
    # æŒ‰ç« èŠ‚åˆ†ç»„
    print("æ­£åœ¨æŒ‰ç« èŠ‚åˆ†ç»„...")
    chapters_dict = group_by_chapters(rows)
    chapter_uids = sorted(chapters_dict.keys())
    
    print(f"å…± {len(chapter_uids)} ä¸ªç« èŠ‚: {chapter_uids}\n")
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = OutlineGenerator(api_key=api_key, role=role)
    
    # æŒ‰ç« èŠ‚åˆ†ç»„ï¼Œæ¯ç»„è‡³å°‘50ä¸ªç¬”è®°
    min_notes_per_group = 50
    
    print("=" * 60)
    print(f"å¼€å§‹å¤„ç†ï¼ˆæ¯ç»„è‡³å°‘ {min_notes_per_group} ä¸ªç¬”è®°ï¼‰")
    print("=" * 60)
    
    # å‡†å¤‡ CSV ç¼“å­˜æ–‡ä»¶è·¯å¾„
    script_dir = Path(__file__).parent  # llm/scripts
    output_dir = script_dir.parent / "output" / "outlines"  # llm/output/outlines
    output_dir.mkdir(parents=True, exist_ok=True)
    cache_csv_file = output_dir / f"{book_id}_outline_blocks.csv"
    
    # åŠ è½½å·²æœ‰çš„ block ç¼“å­˜
    existing_blocks = {}
    existing_blocks_info = {}  # å­˜å‚¨å®Œæ•´çš„ block ä¿¡æ¯ï¼ˆåŒ…æ‹¬ start_chapter, start_note_id ç­‰ï¼‰
    if cache_csv_file.exists():
        print(f"\næ£€æµ‹åˆ°å·²å­˜åœ¨çš„ block ç¼“å­˜æ–‡ä»¶: {cache_csv_file}")
        try:
            with open(cache_csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    block_id = row.get('block_id', '').strip()
                    if block_id:
                        existing_blocks[block_id] = {
                            'html': row.get('html', ''),
                            'markdown': row.get('markdown', ''),
                            'created_at': row.get('created_at', ''),
                            'updated_at': row.get('updated_at', '')
                        }
                        # ä¿å­˜å®Œæ•´çš„ block ä¿¡æ¯ï¼ˆä» CSV åˆ—è¯»å–ï¼Œè€Œä¸æ˜¯ä» block_id è§£æï¼‰
                        existing_blocks_info[block_id] = {
                            'start_chapter': row.get('start_chapter', '').strip(),
                            'start_note_id': row.get('start_note_id', '').strip(),
                            'end_chapter': row.get('end_chapter', '').strip(),
                            'end_note_id': row.get('end_note_id', '').strip()
                        }
            print(f"  å·²åŠ è½½ {len(existing_blocks)} ä¸ªå·²æœ‰ block")
        except Exception as e:
            print(f"  âš ï¸  è¯»å– block ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    # ç¬¬ä¸€æ­¥ï¼šå…ˆç”Ÿæˆæ‰€æœ‰ block åˆ’åˆ†ï¼ˆç¡®å®šæ‰€æœ‰è¦å¤„ç†çš„ blockï¼‰
    print(f"\nç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ‰€æœ‰ block åˆ’åˆ†...")
    all_block_definitions = []  # æ‰€æœ‰ block çš„å®šä¹‰ï¼ˆåŒ…å«ç« èŠ‚ã€ç¬”è®°ç­‰ï¼‰
    
    i = 0
    group_idx = 0
    
    while i < len(chapter_uids):
        # æ”¶é›†å½“å‰ç»„çš„æ‰€æœ‰ç« èŠ‚
        group_chapters = []
        total_notes = 0
        
        # ä»å½“å‰ç« èŠ‚å¼€å§‹ï¼Œç´¯ç§¯åˆ°è‡³å°‘50ä¸ªç¬”è®°
        j = i
        while j < len(chapter_uids) and total_notes < min_notes_per_group:
            chapter_uid = chapter_uids[j]
            chapter_rows = chapters_dict[chapter_uid]
            # ç»Ÿè®¡è¯¥ç« èŠ‚çš„ç¬”è®°æ•°é‡ï¼ˆæœ‰ markText çš„è¡Œï¼‰
            notes_count = len([row for row in chapter_rows if row.get('markText', '').strip()])
            
            group_chapters.append(chapter_uid)
            total_notes += notes_count
            j += 1
        
        if not group_chapters:
            break
        
        group_idx += 1
        
        # æ”¶é›†è¿™ç»„ç« èŠ‚çš„åˆ’çº¿ç¬”è®°å’Œç‚¹è¯„ç¬”è®°ï¼ŒåŒæ—¶æ”¶é›†ç¬”è®° ID
        mark_notes_parts = []  # åˆ’çº¿ç¬”è®°ï¼ˆåŒ…å«ç« èŠ‚æ ‡é¢˜å’Œåˆ’çº¿æ–‡æœ¬ï¼‰
        review_notes_parts = []  # ç‚¹è¯„ç¬”è®°
        
        chapter_names = []
        first_note_id = None  # ç¬¬ä¸€ä¸ªç¬”è®°çš„ ID
        last_note_id = None   # æœ€åä¸€ä¸ªç¬”è®°çš„ ID
        
        for chapter_uid in group_chapters:
            chapter_rows = chapters_dict[chapter_uid]
            chapter_name = chapter_rows[0].get('chapterName', f'ç« èŠ‚{chapter_uid}') if chapter_rows else f'ç« èŠ‚{chapter_uid}'
            chapter_names.append(chapter_name)
            
            # æŒ‰ createTime æ’åºï¼Œç¡®ä¿é¡ºåº
            chapter_rows_sorted = sorted(chapter_rows, key=lambda x: int(x.get('createTime', 0)) if x.get('createTime', '').strip().isdigit() else 0)
            
            # æ·»åŠ ç« èŠ‚æ ‡é¢˜ï¼ˆæ²¡æœ‰ bullet pointï¼‰
            mark_notes_parts.append(chapter_name)
            
            # æ”¶é›†åˆ’çº¿ç¬”è®°ï¼ˆæœ‰ bullet pointï¼‰
            for row in chapter_rows_sorted:
                mark_text = row.get('markText', '').strip()
                if mark_text:
                    mark_notes_parts.append(f"- {mark_text}")
                    # è®°å½•ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç¬”è®° ID
                    note_id = row.get('noteId', '').strip() or row.get('createTime', '').strip()
                    if note_id:
                        if first_note_id is None:
                            first_note_id = note_id
                        last_note_id = note_id
                
                # æ”¶é›†ç‚¹è¯„ç¬”è®°
                review_content = row.get('reviewContent', '').strip()
                if review_content:
                    review_notes_parts.append("ã€åŸæ–‡ã€‘ï¼š" + mark_text + "ã€ç‚¹è¯„ã€‘ï¼š" + review_content)
        
        if not mark_notes_parts:
            i = j
            continue
        
        # ç”Ÿæˆ block_idï¼šå¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°id-ç»“æŸç« èŠ‚å·-ç»“æŸç¬”è®°id
        start_chapter = group_chapters[0]
        end_chapter = group_chapters[-1]
        block_id = f"{start_chapter}-{first_note_id or '0'}-{end_chapter}-{last_note_id or '0'}"
        
        # ä¿å­˜ block å®šä¹‰
        all_block_definitions.append({
            'group_idx': group_idx,
            'block_id': block_id,
            'start_chapter': start_chapter,
            'end_chapter': end_chapter,
            'start_note_id': first_note_id or '',
            'end_note_id': last_note_id or '',
            'group_chapters': group_chapters,
            'chapter_names': chapter_names,
            'mark_notes_parts': mark_notes_parts,
            'review_notes_parts': review_notes_parts,
            'total_notes': total_notes
        })
        
        i = j
    
    print(f"âœ“ å…±åˆ’åˆ†äº† {len(all_block_definitions)} ä¸ª block")
    
    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ CSV ä¸­å·²å­˜åœ¨çš„ blockï¼Œç¡®å®šå“ªäº›éœ€è¦è°ƒç”¨ LLM
    print(f"\nç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ CSV ä¸­å·²å­˜åœ¨çš„ block...")
    
    # å»ºç«‹å·²æœ‰ block çš„ç´¢å¼•ï¼ˆæŒ‰"å¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°id"åˆ†ç»„ï¼Œç”¨äºæŸ¥æ‰¾è¦†ç›–æƒ…å†µï¼‰
    existing_blocks_by_start = {}  # key = "å¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°id", value = list of blocks
    for block_id, block_data in existing_blocks.items():
        # ä» CSV åˆ—è¯»å–ç« èŠ‚ä¿¡æ¯ï¼ˆè€Œä¸æ˜¯ä» block_id è§£æï¼Œå› ä¸º start_note_id å¯èƒ½åŒ…å« '-'ï¼‰
        block_info = existing_blocks_info.get(block_id, {})
        start_chapter = block_info.get('start_chapter', '')
        start_note_id = block_info.get('start_note_id', '')
        end_chapter = block_info.get('end_chapter', '')
        
        start_key = f"{start_chapter}-{start_note_id}"
        if start_key not in existing_blocks_by_start:
            existing_blocks_by_start[start_key] = []
        existing_blocks_by_start[start_key].append({
            'block_id': block_id,
            'start_chapter': start_chapter,
            'start_note_id': start_note_id,
            'end_chapter': end_chapter,
            'block_data': block_data
        })
    
    # ç¡®å®šå“ªäº› block éœ€è¦è°ƒç”¨ LLM
    blocks_to_generate = []  # éœ€è¦è°ƒç”¨ LLM çš„ block
    blocks_to_use_cache = {}  # ä½¿ç”¨ç¼“å­˜çš„ blockï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    blocks_to_update = []  # éœ€è¦è¦†ç›–çš„ blockï¼ˆå¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°idç›¸åŒï¼‰
    
    for block_def in all_block_definitions:
        block_id = block_def['block_id']
        start_chapter = block_def['start_chapter']
        start_note_id = block_def['start_note_id']
        end_chapter = block_def['end_chapter']
        
        # 1. æ£€æŸ¥ç²¾ç¡®åŒ¹é…ï¼ˆblock_id å®Œå…¨ç›¸åŒï¼‰
        if block_id in existing_blocks:
            print(f"  âœ“ Block {block_def['group_idx']} å·²å­˜åœ¨ï¼ˆID: {block_id}ï¼‰ï¼Œå°†ä½¿ç”¨ç¼“å­˜")
            blocks_to_use_cache[block_id] = existing_blocks[block_id]
            continue
        
        # 2. æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆå¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°id ç›¸åŒï¼‰
        start_key = f"{start_chapter}-{start_note_id}"
        if start_key in existing_blocks_by_start:
            # åªè¦å¼€å§‹ç« èŠ‚å·-å¼€å§‹ç¬”è®°idç›¸åŒï¼Œå°±è®¤ä¸ºéœ€è¦è¦†ç›–
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ blockï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
            existing_block_info = existing_blocks_by_start[start_key][0]
            existing_end_chapter = existing_block_info['end_chapter']
            existing_block_id = existing_block_info['block_id']
            
            print(f"  ğŸ”„ Block {block_def['group_idx']} éœ€è¦è¦†ç›–å·²æœ‰ blockï¼ˆ{existing_block_id} -> {block_id}ï¼Œå¼€å§‹ç« èŠ‚: {start_chapter}ï¼Œç»“æŸç« èŠ‚: {existing_end_chapter} -> {end_chapter}ï¼‰")
            blocks_to_update.append({
                'new_block_def': block_def,
                'old_block_id': existing_block_id,
                'old_block_data': existing_block_info['block_data']
            })
            continue
        
        # 3. å®Œå…¨æ–°çš„ blockï¼Œéœ€è¦è°ƒç”¨ LLM
        print(f"  âœ¨ Block {block_def['group_idx']} æ˜¯æ–°çš„ï¼Œéœ€è¦è°ƒç”¨ LLM ç”Ÿæˆ")
        blocks_to_generate.append(block_def)
    
    # æ”¶é›†æ‰€æœ‰æ–°æ‹†åˆ†çš„ blocks çš„ start_keyï¼ˆç”¨äºåˆ¤æ–­å“ªäº›æ—§ block éœ€è¦åˆ é™¤ï¼‰
    new_block_start_keys = set()
    for block_def in all_block_definitions:
        start_chapter = block_def['start_chapter']
        start_note_id = block_def['start_note_id']
        start_key = f"{start_chapter}-{start_note_id}"
        new_block_start_keys.add(start_key)
    
    print(f"\nç»Ÿè®¡ï¼š")
    print(f"  - ä½¿ç”¨ç¼“å­˜: {len(blocks_to_use_cache)} ä¸ª")
    print(f"  - éœ€è¦è¦†ç›–: {len(blocks_to_update)} ä¸ª")
    print(f"  - éœ€è¦ç”Ÿæˆ: {len(blocks_to_generate)} ä¸ª")
    print(f"  - æ–°æ‹†åˆ†çš„ blocks: {len(new_block_start_keys)} ä¸ª")
    
    # ç¬¬ä¸‰æ­¥ï¼šåªå¯¹éœ€è¦ç”Ÿæˆçš„ block è°ƒç”¨ LLM
    print(f"\nç¬¬ä¸‰æ­¥ï¼šè°ƒç”¨ LLM ç”Ÿæˆæ–° block...")
    new_blocks = []  # æ–°ç”Ÿæˆçš„ blockï¼Œç”¨äºä¿å­˜åˆ° CSV
    
    for block_def in blocks_to_generate:
        group_idx = block_def['group_idx']
        block_id = block_def['block_id']
        mark_notes_parts = block_def['mark_notes_parts']
        review_notes_parts = block_def['review_notes_parts']
        chapter_names = block_def['chapter_names']
        
        print(f"\n[ç»„ {group_idx}] å¤„ç†ç« èŠ‚: {block_def['group_chapters'][0]}-{block_def['group_chapters'][-1]}ï¼ˆ{len(block_def['group_chapters'])} ä¸ªç« èŠ‚ï¼Œ{block_def['total_notes']} æ¡ç¬”è®°ï¼‰")
        
        # æ ¼å¼åŒ–åˆ’çº¿ç¬”è®°ï¼ˆç« èŠ‚æ ‡é¢˜å’Œåˆ’çº¿æ–‡æœ¬ï¼Œç”¨ç©ºè¡Œåˆ†éš”ï¼‰
        mark_notes_text = "\n\n".join(mark_notes_parts)
        
        # æ ¼å¼åŒ–ç‚¹è¯„ç¬”è®°ï¼ˆç”¨ç©ºè¡Œåˆ†éš”ï¼‰
        review_notes_text = "\n\n".join(review_notes_parts) if review_notes_parts else "æ— ç‚¹è¯„ç¬”è®°"
        
        print(f"  ç« èŠ‚åç§°: {', '.join(chapter_names)}")
        print(f"  åˆ’çº¿ç¬”è®°æ•°: {len([p for p in mark_notes_parts if p.startswith('-')])}")
        print(f"  ç‚¹è¯„ç¬”è®°æ•°: {len(review_notes_parts)}")
        print(f"  æ­£åœ¨ç”Ÿæˆå¤§çº²ï¼ˆBlock ID: {block_id}ï¼‰...")
        
        # ç”Ÿæˆå¤§çº²ï¼ˆè¿”å›å­—å…¸ï¼ŒåŒ…å« markdown å’Œ htmlï¼‰
        outline_result = generator.generate_outline(mark_notes_text, review_notes_text)
        
        # ä¿å­˜æ–°ç”Ÿæˆçš„ block åˆ°åˆ—è¡¨ï¼ˆç¨åå†™å…¥ CSVï¼‰
        from datetime import datetime
        current_time = datetime.now().isoformat()
        new_blocks.append({
            'block_id': block_id,
            'start_chapter': block_def['start_chapter'],
            'end_chapter': block_def['end_chapter'],
            'start_note_id': block_def['start_note_id'],
            'end_note_id': block_def['end_note_id'],
            'markdown': outline_result.get('markdown', ''),
            'html': outline_result.get('html', ''),
            'created_at': current_time,
            'updated_at': current_time
        })
        
        print(f"  âœ“ å®Œæˆ")
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å… API è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)
    
    # ç¬¬å››æ­¥ï¼šå¤„ç†éœ€è¦è¦†ç›–çš„ blockï¼ˆä¹Ÿéœ€è¦è°ƒç”¨ LLM ç”Ÿæˆæ–°å†…å®¹ï¼‰
    print(f"\nç¬¬å››æ­¥ï¼šå¤„ç†éœ€è¦è¦†ç›–çš„ blockï¼ˆè°ƒç”¨ LLM ç”Ÿæˆæ–°å†…å®¹ï¼‰...")
    
    for update_info in blocks_to_update:
        block_def = update_info['new_block_def']
        old_block_id = update_info['old_block_id']
        group_idx = block_def['group_idx']
        block_id = block_def['block_id']
        mark_notes_parts = block_def['mark_notes_parts']
        review_notes_parts = block_def['review_notes_parts']
        chapter_names = block_def['chapter_names']
        
        print(f"\n[ç»„ {group_idx}] å¤„ç†ç« èŠ‚: {block_def['group_chapters'][0]}-{block_def['group_chapters'][-1]}ï¼ˆè¦†ç›– {old_block_id}ï¼‰")
        
        # æ ¼å¼åŒ–åˆ’çº¿ç¬”è®°
        mark_notes_text = "\n\n".join(mark_notes_parts)
        review_notes_text = "\n\n".join(review_notes_parts) if review_notes_parts else "æ— ç‚¹è¯„ç¬”è®°"
        
        print(f"  ç« èŠ‚åç§°: {', '.join(chapter_names)}")
        print(f"  åˆ’çº¿ç¬”è®°æ•°: {len([p for p in mark_notes_parts if p.startswith('-')])}")
        print(f"  ç‚¹è¯„ç¬”è®°æ•°: {len(review_notes_parts)}")
        print(f"  æ­£åœ¨ç”Ÿæˆå¤§çº²ï¼ˆBlock ID: {block_id}ï¼Œå°†è¦†ç›– {old_block_id}ï¼‰...")
        
        # ç”Ÿæˆå¤§çº²ï¼ˆè¿”å›å­—å…¸ï¼ŒåŒ…å« markdown å’Œ htmlï¼‰
        outline_result = generator.generate_outline(mark_notes_text, review_notes_text)
        
        # ä¿å­˜æ–°ç”Ÿæˆçš„ blockï¼ˆä¿ç•™åŸæœ‰çš„ created_atï¼‰
        from datetime import datetime
        current_time = datetime.now().isoformat()
        old_block_data = update_info['old_block_data']
        new_blocks.append({
            'block_id': block_id,
            'start_chapter': block_def['start_chapter'],
            'end_chapter': block_def['end_chapter'],
            'start_note_id': block_def['start_note_id'],
            'end_note_id': block_def['end_note_id'],
            'markdown': outline_result.get('markdown', ''),
            'html': outline_result.get('html', ''),
            'created_at': old_block_data.get('created_at', current_time),  # ä¿ç•™åŸæœ‰çš„ created_at
            'updated_at': current_time
        })
        
        print(f"  âœ“ å®Œæˆï¼ˆå°†è¦†ç›– {old_block_id}ï¼‰")
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å… API è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)
    
    # ç¬¬äº”æ­¥ï¼šæ„å»ºæ‰€æœ‰ block çš„ç»“æœï¼ˆç¼“å­˜ + æ–°ç”Ÿæˆçš„ï¼‰
    print(f"\nç¬¬äº”æ­¥ï¼šæ„å»ºæ‰€æœ‰ block çš„ç»“æœ...")
    all_markdown_parts = []
    all_html_parts = []
    
    for block_def in all_block_definitions:
        block_id = block_def['block_id']
        group_idx = block_def['group_idx']
        group_chapters = block_def['group_chapters']
        chapter_names = block_def['chapter_names']
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªç»“æœ
        if block_id in blocks_to_use_cache:
            # ä½¿ç”¨ç¼“å­˜
            cached_block = blocks_to_use_cache[block_id]
            outline_result = {
                'markdown': cached_block.get('markdown', ''),
                'html': cached_block.get('html', '')
            }
        else:
            # ä½¿ç”¨æ–°ç”Ÿæˆçš„ï¼ˆåœ¨ new_blocks ä¸­æŸ¥æ‰¾ï¼‰
            found_new_block = None
            for new_block in new_blocks:
                if new_block['block_id'] == block_id:
                    found_new_block = new_block
                    break
            
            if found_new_block:
                outline_result = {
                    'markdown': found_new_block.get('markdown', ''),
                    'html': found_new_block.get('html', '')
                }
            else:
                # ä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                outline_result = {'markdown': '', 'html': ''}
        
        # æ·»åŠ ç»„æ ‡é¢˜
        group_title_md = f"# ç¬¬ {group_idx} ç»„ï¼šç« èŠ‚ {group_chapters[0]}-{group_chapters[-1]}\n\n"
        group_title_md += f"**ç« èŠ‚åç§°**: {', '.join(chapter_names)}\n\n"
        group_title_md += f"**ç« èŠ‚ID**: {', '.join(map(str, group_chapters))}\n\n"
        group_title_md += "---\n\n"
        
        group_title_html = f"<h1>ç¬¬ {group_idx} ç»„ï¼šç« èŠ‚ {group_chapters[0]}-{group_chapters[-1]}</h1>\n"
        group_title_html += f"<p><strong>ç« èŠ‚åç§°</strong>: {', '.join(chapter_names)}</p>\n"
        group_title_html += f"<p><strong>ç« èŠ‚ID</strong>: {', '.join(map(str, group_chapters))}</p>\n"
        group_title_html += "<hr>\n"
        
        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
        all_markdown_parts.append(group_title_md + outline_result.get('markdown', ''))
        all_html_parts.append(group_title_html + outline_result.get('html', ''))
        
    # å…³é—­å®¢æˆ·ç«¯
    generator.close()
    
    # ä¿å­˜æ‰€æœ‰ block åˆ° CSVï¼ˆå·²æœ‰çš„ + æ–°ç”Ÿæˆçš„ï¼‰
    if new_blocks or existing_blocks:
        print(f"\næ­£åœ¨ä¿å­˜ block ç¼“å­˜åˆ° CSV...")
        from datetime import datetime
        current_time = datetime.now().isoformat()
        
        # æ”¶é›†éœ€è¦è¦†ç›–çš„æ—§ block_idï¼ˆç”¨äºåˆ é™¤ï¼‰
        old_block_ids_to_remove = set()
        for update_info in blocks_to_update:
            old_block_ids_to_remove.add(update_info['old_block_id'])
        
        # å…ˆæ·»åŠ å·²æœ‰çš„ blockï¼ˆé™¤äº†è¢«è¦†ç›–çš„ï¼Œä»¥åŠä¸åœ¨æ–°æ‹†åˆ† blocks ä¸­çš„ï¼‰
        all_blocks_to_save = {}
        removed_old_blocks = []  # è®°å½•è¢«åˆ é™¤çš„æ—§ block
        
        for block_id, block_data in existing_blocks.items():
            # å¦‚æœè¿™ä¸ª block è¢«è¦†ç›–äº†ï¼Œè·³è¿‡
            if block_id in old_block_ids_to_remove:
                continue
            
            # ä» CSV åˆ—è¯»å–ç« èŠ‚ä¿¡æ¯ï¼ˆè€Œä¸æ˜¯ä» block_id è§£æï¼‰
            block_info_from_csv = existing_blocks_info.get(block_id, {})
            start_chapter = block_info_from_csv.get('start_chapter', '')
            start_note_id = block_info_from_csv.get('start_note_id', '')
            start_key = f"{start_chapter}-{start_note_id}"
            
            # å¦‚æœè¿™ä¸ª block çš„ start_key ä¸åœ¨æ–°æ‹†åˆ†çš„ blocks ä¸­ï¼Œåˆ é™¤å®ƒ
            if start_key not in new_block_start_keys:
                removed_old_blocks.append(block_id)
                continue
            
            block_info = {
                'block_id': block_id,
                'start_chapter': start_chapter,
                'end_chapter': block_info_from_csv.get('end_chapter', ''),
                'start_note_id': start_note_id,
                'end_note_id': block_info_from_csv.get('end_note_id', ''),
                'markdown': block_data.get('markdown', ''),
                'html': block_data.get('html', ''),
                'created_at': block_data.get('created_at', current_time),
                'updated_at': current_time  # æ›´æ–°æ—¶é—´æˆ³
            }
            
            all_blocks_to_save[block_id] = block_info
        
        # æŠ¥å‘Šåˆ é™¤çš„æ—§ block
        if removed_old_blocks:
            print(f"  ğŸ—‘ï¸  åˆ é™¤äº† {len(removed_old_blocks)} ä¸ªä¸åœ¨æ–°æ‹†åˆ† blocks ä¸­çš„æ—§ block")
            for removed_id in removed_old_blocks[:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ª
                print(f"     - {removed_id}")
            if len(removed_old_blocks) > 5:
                print(f"     ... è¿˜æœ‰ {len(removed_old_blocks) - 5} ä¸ª")
        
        # æ·»åŠ æ‰€æœ‰æ–°ç”Ÿæˆçš„ blockï¼ˆåŒ…æ‹¬è¦†ç›–çš„å’Œæ–°å¢çš„ï¼‰
        for new_block in new_blocks:
            all_blocks_to_save[new_block['block_id']] = new_block
        
        updated_count = len(blocks_to_update)
        if updated_count > 0:
            print(f"  âœ“ æ›´æ–°äº† {updated_count} ä¸ª blockï¼ˆç”¨æ–°ç”Ÿæˆçš„ block è¦†ç›–äº†æ»¡è¶³æ¡ä»¶çš„å·²æœ‰ blockï¼‰")
        else:
            print(f"  âœ“ æ²¡æœ‰éœ€è¦æ›´æ–°çš„ block")
        
        # ä¿å­˜åˆ° CSV
        fieldnames = ['block_id', 'start_chapter', 'end_chapter', 'start_note_id', 'end_note_id', 'markdown', 'html', 'created_at', 'updated_at']
        try:
            with open(cache_csv_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                # æŒ‰å¼€å§‹ç« èŠ‚ä»å°åˆ°å¤§æ’åº
                sorted_blocks = sorted(all_blocks_to_save.values(), key=lambda x: (
                    int(x.get('start_chapter', 0)) if str(x.get('start_chapter', '0')).isdigit() else 0,
                    x.get('start_note_id', '')
                ))
                for block in sorted_blocks:
                    writer.writerow(block)
            print(f"âœ“ å·²ä¿å­˜ {len(all_blocks_to_save)} ä¸ª block åˆ° {cache_csv_file}")
            if new_blocks:
                new_count = len(new_blocks) - updated_count
                if updated_count > 0:
                    print(f"  - æ–°å¢: {new_count} ä¸ª")
                    print(f"  - æ›´æ–°: {updated_count} ä¸ªï¼ˆè¦†ç›–å·²æœ‰ blockï¼‰")
                else:
                    print(f"  - æ–°å¢: {len(new_blocks)} ä¸ª")
                remaining_existing = len(existing_blocks) - updated_count - len(removed_old_blocks)
                if remaining_existing > 0:
                    print(f"  - å·²æœ‰: {remaining_existing} ä¸ªï¼ˆå·²ä¿ç•™ï¼‰")
                if removed_old_blocks:
                    print(f"  - åˆ é™¤: {len(removed_old_blocks)} ä¸ªï¼ˆä¸åœ¨æ–°æ‹†åˆ† blocks ä¸­ï¼‰")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ block ç¼“å­˜å¤±è´¥: {e}")
    
    # ä» CSV é‡æ–°è¯»å–æ‰€æœ‰ blockï¼ŒæŒ‰é¡ºåºæ±‡æ€»ï¼ˆç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
    print(f"\næ­£åœ¨ä» CSV æ±‡æ€»æ‰€æœ‰ block...")
    all_blocks_sorted = []
    try:
        if cache_csv_file.exists():
            with open(cache_csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    all_blocks_sorted.append(row)
            # æŒ‰å¼€å§‹ç« èŠ‚ä»å°åˆ°å¤§æ’åº
            all_blocks_sorted.sort(key=lambda x: (
                int(x.get('start_chapter', 0)) if str(x.get('start_chapter', '0')).isdigit() else 0,
                x.get('start_note_id', '')
            ))
            print(f"  ä» CSV åŠ è½½äº† {len(all_blocks_sorted)} ä¸ª block")
    except Exception as e:
        print(f"  âš ï¸  ä» CSV è¯»å– block å¤±è´¥: {e}")
        # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®
        all_blocks_sorted = []
        for block_id in sorted(existing_blocks.keys()):
            block = existing_blocks[block_id]
            block['block_id'] = block_id
            all_blocks_sorted.append(block)
        for block in sorted(new_blocks, key=lambda x: (
            int(x.get('start_chapter', 0)) if str(x.get('start_chapter', '0')).isdigit() else 0,
            x.get('start_note_id', '')
        )):
            all_blocks_sorted.append(block)
    
    # é‡æ–°æ„å»º markdown å’Œ HTMLï¼ˆä» CSV ä¸­çš„ blockï¼‰
    all_markdown_parts_from_csv = []
    all_html_parts_from_csv = []
    
    group_idx_from_csv = 0
    for block in all_blocks_sorted:
        group_idx_from_csv += 1
        start_chapter = block.get('start_chapter', '')
        end_chapter = block.get('end_chapter', '')
    
        # æ·»åŠ ç»„æ ‡é¢˜
        group_title_md = f"# ç¬¬ {group_idx_from_csv} ç»„ï¼šç« èŠ‚ {start_chapter}-{end_chapter}\n\n"
        group_title_md += "---\n\n"
        
        group_title_html = f"<h1>ç¬¬ {group_idx_from_csv} ç»„ï¼šç« èŠ‚ {start_chapter}-{end_chapter}</h1>\n"
        group_title_html += "<hr>\n"
        
        all_markdown_parts_from_csv.append(group_title_md + block.get('markdown', ''))
        all_html_parts_from_csv.append(group_title_html + block.get('html', ''))
    
    # åˆå¹¶æ‰€æœ‰å¤§çº²
    final_markdown = f"# {book_title} - å­¦ä¹ å¤§çº²\n\n"
    final_markdown += f"**é¢†åŸŸ**: {field}\n\n"
    final_markdown += "---\n\n"
    final_markdown += "\n\n".join(all_markdown_parts_from_csv)
    
    # æ¸…ç† HTML ä¸­å¯èƒ½æ®‹ç•™çš„ Markdown ä»£ç å—è¯­æ³•
    cleaned_html_parts = []
    for html_part in all_html_parts_from_csv:
        # ç§»é™¤ Markdown ä»£ç å—æ ‡è®°ï¼ˆä½†ä¿ç•™ HTML æ ‡ç­¾å†…çš„å†…å®¹ï¼‰
        cleaned = re.sub(r'```[a-z]*\n?', '', html_part)
        cleaned = re.sub(r'\n?```', '', cleaned)
        cleaned_html_parts.append(cleaned)
    
    final_html = f"<html><head><meta charset='utf-8'><title>{book_title} - å­¦ä¹ å¤§çº²</title></head><body>\n"
    final_html += f"<h1>{book_title} - å­¦ä¹ å¤§çº²</h1>\n"
    final_html += f"<p><strong>é¢†åŸŸ</strong>: {field}</p>\n"
    final_html += "<hr>\n"
    final_html += "\n".join(cleaned_html_parts)
    final_html += "</body></html>"
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if output_file is None:
        script_dir = Path(__file__).parent  # llm/scripts
        output_dir = script_dir.parent / "output" / "outlines"  # llm/output/outlines
        output_dir.mkdir(parents=True, exist_ok=True)
        base_name = book_id
        markdown_file = str(output_dir / f"{base_name}_outline.md")
        html_file = str(output_dir / f"{base_name}_outline.html")
    else:
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨å®ƒä½œä¸ºåŸºç¡€åç§°
        output_path = Path(output_file)
        base_name = output_path.stem
        output_dir = output_path.parent
        markdown_file = str(output_dir / f"{base_name}.md")
        html_file = str(output_dir / f"{base_name}.html")
    
    print(f"\næ­£åœ¨ä¿å­˜æ–‡ä»¶...")
    
    # ä¿å­˜ Markdown æ–‡ä»¶
    markdown_path = Path(markdown_file)
    markdown_path.parent.mkdir(parents=True, exist_ok=True)
    with open(markdown_path, 'w', encoding='utf-8') as f:
        f.write(final_markdown)
    print(f"âœ“ Markdown å·²ä¿å­˜åˆ°: {markdown_file}")
    
    # ä¿å­˜ HTML æ–‡ä»¶
    html_path = Path(html_file)
    html_path.parent.mkdir(parents=True, exist_ok=True)
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(final_html)
    print(f"âœ“ HTML å·²ä¿å­˜åˆ°: {html_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å­¦ä¹ å¤§çº²ç”Ÿæˆå·¥å…·ï¼šä»ç¬”è®° CSV æ–‡ä»¶ä¸­æŒ‰ç« èŠ‚åˆ†ç»„ï¼Œä½¿ç”¨ Gemini API ç”Ÿæˆå­¦ä¹ å¤§çº²',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # ä½¿ç”¨ bookID
  python generate_outline.py --book-id 3300089819
  python generate_outline.py --book-id 3300089819 --output llm/output/outlines/book_outline.md
  
  # ä½¿ç”¨ä¹¦å
  python generate_outline.py --book-name "ä¹¦å"
  python generate_outline.py --book-name "ä¹¦å" --role å­¦ä¹ è€…
  
  # å…ˆé‡æ–° fetch æ•°æ®ï¼Œå†ç”Ÿæˆ outline
  python generate_outline.py --book-name "ä¹¦å" --fetch
  python generate_outline.py --book-id 3300089819 --fetch
        """
    )
    
    # ä¹¦åå’Œ bookID äºŒé€‰ä¸€
    book_group = parser.add_mutually_exclusive_group(required=True)
    book_group.add_argument('--book-name', '--book-title', dest='book_title', type=str,
                           help='ä¹¦ç±åç§°')
    book_group.add_argument('--book-id', '--id', dest='book_id', type=str,
                           help='ä¹¦ç±ID')
    
    parser.add_argument('--output', '--output-file', dest='output_file', type=str, default=None,
                       help='è¾“å‡ºçš„ Markdown/HTML æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--role', type=str, default='å­¦ä¹ è€…',
                       help='è§’è‰²ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º"å­¦ä¹ è€…"ï¼‰')
    parser.add_argument('--api-key', type=str,
                       help='Gemini API å¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä»ç¯å¢ƒå˜é‡ GEMINI_API_KEY æˆ– GOOGLE_API_KEY è¯»å–ï¼‰')
    parser.add_argument('--fetch', '--refresh-data', dest='fetch_data', action='store_true',
                       help='åœ¨ç”Ÿæˆ outline ä¹‹å‰ï¼Œå…ˆé‡æ–° fetch ç¬”è®°æ•°æ®ï¼ˆè°ƒç”¨ wereader/fetch.pyï¼‰')
    
    args = parser.parse_args()
    
    # è·å– API å¯†é’¥ï¼ˆä¼˜å…ˆä»å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä»ç¯å¢ƒå˜é‡ï¼‰
    api_key = args.api_key or os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')
    
    if not api_key:
        print("é”™è¯¯ï¼šè¯·è®¾ç½® GEMINI_API_KEY æˆ– GOOGLE_API_KEY ç¯å¢ƒå˜é‡ï¼Œæˆ–ä½¿ç”¨ --api-key å‚æ•°")
        sys.exit(1)
    
    try:
        process_csv_file(
            book_id=args.book_id,
            book_title=args.book_title,
            output_file=args.output_file,
            api_key=api_key,
            role=args.role,
            fetch_data=args.fetch_data
        )
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\né”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

